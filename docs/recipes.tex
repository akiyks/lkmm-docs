\section{Recipes}

This document provides ``recipes'', that is, litmus tests for commonly
occurring situations, as well as a few that illustrate subtly broken but
attractive nuisances.
Many of these recipes include example code from v5.7 of the Linux kernel.

The first section covers simple special cases, the second section
takes off the training wheels to cover more involved examples,
and the third section provides a few rules of thumb.


\subsection{Simple special cases}

This section presents two simple special cases, the first being where
there is only one CPU or only one memory location is accessed, and the
second being use of that old concurrency workhorse, locking.


\subsubsection{Single CPU or single memory location}

If there is only one CPU on the one hand or only one variable
on the other, the code will execute in order.
There are (as usual) some things to be careful of:

\begin{enumerate}
\item	Some aspects of the C language are unordered.
	For example, in the expression \qco{f(x) + g(y)}, the order in
	which \co{f} and \co{g} are called is not defined; the object
	code is allowed to use either order or even to interleave the
	computations.

\item	Compilers are permitted to use the ``as-if'' rule.
	That is, a compiler can emit whatever code it likes for normal accesses,
	as long as the results of a single-threaded execution appear
	just as if the compiler had followed all the relevant rules.
	To see this, compile with a high level of optimization and run
	the debugger on the resulting binary.

\item	If there is only one variable but multiple CPUs, that variable
	must be properly aligned and all accesses to that variable must
	be full sized.
	Variables that straddle cachelines or pages void your full-ordering
	warranty, as do undersized accesses that load from or store to only
	part of the variable.

\item	If there are multiple CPUs, accesses to shared variables should
	use \co{READ_ONCE()} and \co{WRITE_ONCE()} or stronger to prevent
	load/store tearing, load/store fusing, and invented loads and stores.
	There are exceptions to this rule, including:

	\begin{enumerate}
	\item	When there is no possibility of a given shared variable
		being updated by some other CPU, for example, while
		holding the update-side lock, reads from that variable
		need not use \co{READ_ONCE()}.

	\item	When there is no possibility of a given shared variable
		being either read or updated by other CPUs, for example,
		when running during early boot, reads from that variable
		need not use \co{READ_ONCE()} and writes to that variable
		need not use \co{WRITE_ONCE()}.
	\end{enumerate}
\end{enumerate}


\subsubsection{Locking}

Locking is well-known and straightforward, at least if you don't think
about it too hard.
And the basic rule is indeed quite simple:
Any CPU that has acquired a given lock sees any changes previously seen
or made by any CPU before it released that same lock.
Note that this statement is a bit stronger than ``Any CPU holding a given
lock sees all changes made by any CPU during the time that CPU was holding
this same lock''.
For example, consider the following pair of code fragments:

\begin{VerbatimU}
	/* See MP+polocks.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		spin_lock(&mylock);
		WRITE_ONCE(y, 1);
		spin_unlock(&mylock);
	}

	void CPU1(void)
	{
		spin_lock(&mylock);
		r0 = READ_ONCE(y);
		spin_unlock(&mylock);
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

The basic rule guarantees that if \co{CPU0()} acquires \co{mylock} before
\co{CPU1()}, then both \co{r0} and \co{r1} must be set to the value~1.
This also has the consequence that if the final value of \co{r0} is equal to~1, then the final value of \co{r1} must also be equal to~1.
In contrast, the weaker rule would say nothing about the final value of \co{r1}.

The converse to the basic rule also holds, as illustrated by the
following litmus test:

\begin{VerbatimU}
	/* See MP+porevlocks.litmus. */
	void CPU0(void)
	{
		r0 = READ_ONCE(y);
		spin_lock(&mylock);
		r1 = READ_ONCE(x);
		spin_unlock(&mylock);
	}

	void CPU1(void)
	{
		spin_lock(&mylock);
		WRITE_ONCE(x, 1);
		spin_unlock(&mylock);
		WRITE_ONCE(y, 1);
	}
\end{VerbatimU}

This converse to the basic rule guarantees that if \co{CPU0()} acquires
\co{mylock} before \co{CPU1()}, then both \co{r0} and \co{r1} must be set
to the value~0.
This also has the consequence that if the final value of \co{r1} is equal
to~0, then the final value of \co{r0} must also be equal to~0.
In contrast, the weaker rule would say nothing about the final value of \co{r0}.

These examples show only a single pair of CPUs, but the effects of the
locking basic rule extend across multiple acquisitions of a given lock
across multiple CPUs.

However, it is not necessarily the case that accesses ordered by
locking will be seen as ordered by CPUs not holding that lock.
Consider this example:

\begin{VerbatimU}
	/* See Z6.0+pooncelock+pooncelock+pombonce.litmus. */
	void CPU0(void)
	{
		spin_lock(&mylock);
		WRITE_ONCE(x, 1);
		WRITE_ONCE(y, 1);
		spin_unlock(&mylock);
	}

	void CPU1(void)
	{
		spin_lock(&mylock);
		r0 = READ_ONCE(y);
		WRITE_ONCE(z, 1);
		spin_unlock(&mylock);
	}

	void CPU2(void)
	{
		WRITE_ONCE(z, 2);
		smp_mb();
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

Counter-intuitive though it might be, it is quite possible to have
the final value of \co{r0} be~1, the final value of~\co{z} be~2, and the final
value of \co{r1} be~0.
The reason for this surprising outcome is that \co{CPU2()} never acquired
the lock, and thus did not benefit from the lock's ordering properties.

Ordering can be extended to CPUs not holding the lock by careful use
of \co{smp_mb__after_spinlock()}:

\begin{VerbatimU}
	/* See Z6.0+pooncelock+poonceLock+pombonce.litmus. */
	void CPU0(void)
	{
		spin_lock(&mylock);
		WRITE_ONCE(x, 1);
		WRITE_ONCE(y, 1);
		spin_unlock(&mylock);
	}

	void CPU1(void)
	{
		spin_lock(&mylock);
		smp_mb__after_spinlock();
		r0 = READ_ONCE(y);
		WRITE_ONCE(z, 1);
		spin_unlock(&mylock);
	}

	void CPU2(void)
	{
		WRITE_ONCE(z, 2);
		smp_mb();
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

This addition of \co{smp_mb__after_spinlock()} strengthens the lock acquisition
sufficiently to rule out the counter-intuitive outcome.


\subsection{Taking off the training wheels}

This section looks at more complex examples, including message passing,
load buffering, release-acquire chains, store buffering.
Many classes of litmus tests have abbreviated names, which may be found
here: \url{https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test6.pdf}


\subsubsection{Message passing (MP)}

The MP pattern has one CPU execute a pair of stores to a pair of variables
and another CPU execute a pair of loads from this same pair of variables,
but in the opposite order.
The goal is to avoid the counter-intuitive outcome in which the first load
sees the value written by the second store but the second load does not see
the value written by the first store.
In the absence of any ordering, this goal may not be met, as can be seen
in the \path{MP+poonceonces.litmus} litmus test.
This section therefore looks at a number of ways of meeting this goal.


\paragraph{Release and acquire}

Use of \co{smp_store_release()} and \co{smp_load_acquire()} is one way to force
the desired MP ordering.  The general approach is shown below:

\begin{VerbatimU}
	/* See MP+pooncerelease+poacquireonce.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		smp_store_release(&y, 1);
	}

	void CPU1(void)
	{
		r0 = smp_load_acquire(&y);
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

The \co{smp_store_release()} macro orders any prior accesses against the
store, while the \co{smp_load_acquire()} macro orders the load against any
subsequent accesses.
Therefore, if the final value of \co{r0} is the value~1, the final value
of \co{r1} must also be the value~1.

The \co{init_stack_slab()} function in \path{lib/stackdepot.c} uses
release-acquire in this way to safely initialize of a slab of the stack.
Working out the mutual-exclusion design is left as an exercise for the reader.


\paragraph{Assign and dereference}

Use of \co{rcu_assign_pointer()} and \co{rcu_dereference()} is quite similar
to the use of \co{smp_store_release()} and \co{smp_load_acquire()}, except
that both \co{rcu_assign_pointer()} and \co{rcu_dereference()} operate on
RCU-protected pointers.
The general approach is shown below:

\begin{VerbatimU}
	/* See MP+onceassign+derefonce.litmus. */
	int z;
	int *y = &z;
	int x;

	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		rcu_assign_pointer(y, &x);
	}

	void CPU1(void)
	{
		rcu_read_lock();
		r0 = rcu_dereference(y);
		r1 = READ_ONCE(*r0);
		rcu_read_unlock();
	}
\end{VerbatimU}

In this example, if the final value of \co{r0} is \co{&x} then the final
value of \co{r1} must be~1.

The \co{rcu_assign_pointer()} macro has the same ordering properties as does
\co{smp_store_release()}, but the \co{rcu_dereference()} macro orders the
load only against later accesses that depend on the value loaded.
A dependency is present if the value loaded determines the address of
a later access (address dependency, as shown above), the value written
by a later store (data dependency), or whether or not a later store is
executed in the first place (control dependency).
Note that the term ``data dependency'' is sometimes casually used to
cover both address and data dependencies.

In \path{lib/math/prime_numbers.c}, the \co{expand_to_next_prime()}
function invokes \co{rcu_assign_pointer()}, and the \co{next_prime_number()}
function invokes \co{rcu_dereference()}.
This combination mediates access to a bit vector that is expanded as
additional primes are needed.


\paragraph{Write and read memory barriers}

It is usually better to use \co{smp_store_release()} instead of \co{smp_wmb()}
and to use \co{smp_load_acquire()} instead of \co{smp_rmb()}.
However, the older \co{smp_wmb()} and \co{smp_rmb()} APIs are still heavily
used, so it is important to understand their use cases.
The general approach is shown below:

\begin{VerbatimU}
	/* See MP+fencewmbonceonce+fencermbonceonce.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		smp_wmb();
		WRITE_ONCE(y, 1);
	}

	void CPU1(void)
	{
		r0 = READ_ONCE(y);
		smp_rmb();
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

The \co{smp_wmb()} macro orders prior stores against later stores, and the
\co{smp_rmb()} macro orders prior loads against later loads.
Therefore, if the final value of \co{r0} is~1, the final value of \co{r1}
must also be~1.

The \co{xlog_state_switch_iclogs()} function in \path{fs/xfs/xfs_log.c}
contains the following write-side code fragment:

\begin{VerbatimU}
	log->l_curr_block -= log->l_logBBsize;
	ASSERT(log->l_curr_block >= 0);
	smp_wmb();
	log->l_curr_cycle++;
\end{VerbatimU}

And the \co{xlog_valid_lsn()} function in \path{fs/xfs/xfs_log_priv.h}
contains the corresponding read-side code fragment:

\begin{VerbatimU}
	cur_cycle = READ_ONCE(log->l_curr_cycle);
	smp_rmb();
	cur_block = READ_ONCE(log->l_curr_block);
\end{VerbatimU}

Alternatively, consider the following comment in function
\co{perf_output_put_handle()} in \path{kernel/events/ring_buffer.c}:

\begin{VerbatimU}
	 *   kernel                             user
	 *
	 *   if (LOAD ->data_tail) {            LOAD ->data_head
	 *                      (A)             smp_rmb()       (C)
	 *      STORE $data                     LOAD $data
	 *      smp_wmb()       (B)             smp_mb()        (D)
	 *      STORE ->data_head               STORE ->data_tail
	 *   }
\end{VerbatimU}

The B/C pairing is an example of the MP pattern using \co{smp_wmb()} on the
write side and \co{smp_rmb()} on the read side.

Of course, given that \co{smp_mb()} is strictly stronger than either
\co{smp_wmb()} or \co{smp_rmb()}, any code fragment that would work with
\co{smp_rmb()} and \co{smp_wmb()} would also work with \co{smp_mb()}
replacing either or both of the weaker barriers.


\subsubsection{Load buffering (LB)}

The LB pattern has one CPU load from one variable and then store to a
second, while another CPU loads from the second variable and then stores
to the first.
The goal is to avoid the counter-intuitive situation where each load
reads the value written by the other CPU's store.
In the absence of any ordering it is quite possible that this may happen,
as can be seen in the \path{LB+poonceonces.litmus} litmus test.

One way of avoiding the counter-intuitive outcome is through the use of a
control dependency paired with a full memory barrier:

\begin{VerbatimU}
	/* See LB+fencembonceonce+ctrlonceonce.litmus. */
	void CPU0(void)
	{
		r0 = READ_ONCE(x);
		if (r0)
			WRITE_ONCE(y, 1);
	}

	void CPU1(void)
	{
		r1 = READ_ONCE(y);
		smp_mb();
		WRITE_ONCE(x, 1);
	}
\end{VerbatimU}

This pairing of a control dependency in \co{CPU0()} with a full memory
barrier in \co{CPU1()} prevents \co{r0} and \co{r1} from both ending up
equal to~1.

The A/D pairing from the ring-buffer use case shown earlier also
illustrates LB.  Here is a repeat of the comment in
\co{perf_output_put_handle()} in \path{kernel/events/ring_buffer.c},
showing a control dependency on the kernel side and a full memory barrier on
the user side:

\begin{VerbatimU}
	 *   kernel                             user
	 *
	 *   if (LOAD ->data_tail) {            LOAD ->data_head
	 *                      (A)             smp_rmb()       (C)
	 *      STORE $data                     LOAD $data
	 *      smp_wmb()       (B)             smp_mb()        (D)
	 *      STORE ->data_head               STORE ->data_tail
	 *   }
	 *
	 * Where A pairs with D, and B pairs with C.
\end{VerbatimU}

The kernel's control dependency between the load from \co{->data_tail}
and the store to data combined with the user's full memory barrier
between the load from data and the store to \co{->data_tail} prevents
the counter-intuitive outcome where the kernel overwrites the data
before the user gets done loading it.


\subsubsection{Release-acquire chains}

Release-acquire chains are a low-overhead, flexible, and easy-to-use
method of maintaining order.
However, they do have some limitations that need to be fully understood.
Here is an example that maintains order:

\begin{VerbatimU}
	/* See ISA2+pooncerelease+poacquirerelease+poacquireonce.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		smp_store_release(&y, 1);
	}

	void CPU1(void)
	{
		r0 = smp_load_acquire(y);
		smp_store_release(&z, 1);
	}

	void CPU2(void)
	{
		r1 = smp_load_acquire(z);
		r2 = READ_ONCE(x);
	}
\end{VerbatimU}

In this case, if \co{r0} and \co{r1} both have final values of~1, then \co{r2}
must also have a final value of~1.

The ordering in this example is stronger than it needs to be.
For example, ordering would still be preserved if \co{CPU1()}'s
\co{smp_load_acquire()} invocation was replaced with \co{READ_ONCE()}.

It is tempting to assume that \co{CPU0()}'s store to~\co{x} is globally ordered
before \co{CPU1()}'s store to~\co{z}, but this is not the case:

\begin{VerbatimU}
	/* See Z6.0+pooncerelease+poacquirerelease+mbonceonce.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		smp_store_release(&y, 1);
	}

	void CPU1(void)
	{
		r0 = smp_load_acquire(y);
		smp_store_release(&z, 1);
	}

	void CPU2(void)
	{
		WRITE_ONCE(z, 2);
		smp_mb();
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

One might hope that if the final value of \co{r0} is~1 and the final value
of~\co{z} is~2, then the final value of \co{r1} must also be~1, but it
really is possible for \co{r1} to have the final value of~0.
The reason, of course, is that in this version, \co{CPU2()} is not part
of the release-acquire chain.
This situation is accounted for in the rules of thumb below.

Despite this limitation, release-acquire chains are low-overhead as
well as simple and powerful, at least as memory-ordering mechanisms go.


\subsubsection{Store buffering}

Store buffering can be thought of as upside-down load buffering, so
that one CPU first stores to one variable and then loads from a second,
while another CPU stores to the second variable and then loads from the
first.
Preserving order requires nothing less than full barriers:

\begin{VerbatimU}
	/* See SB+fencembonceonces.litmus. */
	void CPU0(void)
	{
		WRITE_ONCE(x, 1);
		smp_mb();
		r0 = READ_ONCE(y);
	}

	void CPU1(void)
	{
		WRITE_ONCE(y, 1);
		smp_mb();
		r1 = READ_ONCE(x);
	}
\end{VerbatimU}

Omitting either \co{smp_mb()} will allow both \co{r0} and \co{r1} to have final
values of~0, but providing both full barriers as shown above prevents
this counter-intuitive outcome.

This pattern most famously appears as part of Dekker's locking
algorithm, but it has a much more practical use within the Linux kernel
of ordering wakeups.
The following comment taken from \co{waitqueue_active()} in
\path{include/linux/wait.h} shows the canonical pattern:

\begin{VerbatimU}
* CPU0 - waker                    CPU1 - waiter
*
*                                 for (;;) {
* @cond = true;                     prepare_to_wait(&wq_head, &wait, state);
* smp_mb();                         // smp_mb() from set_current_state()
* if (waitqueue_active(wq_head))         if (@cond)
*   wake_up(wq_head);                      break;
*                                   schedule();
*                                 }
*                                 finish_wait(&wq_head, &wait);
\end{VerbatimU}

On \co{CPU0}, the store is to \co{@cond} and the load is in \co{waitqueue_active()}.
On \co{CPU1}, \co{prepare_to_wait()} contains both a store to \co{wq_head}
and a call to \co{set_current_state()}, which contains an \co{smp_mb()} barrier;
the load is \qco{if (@cond)}.
The full barriers prevent the undesirable outcome where \co{CPU1} puts the
waiting task to sleep and \co{CPU0} fails to wake it up.

Note that use of locking can greatly simplify this pattern.


\subsection{Rules of thumb}

There might seem to be no pattern governing what ordering primitives are
needed in which situations, but this is not the case.
There is a pattern based on the relation between the accesses linking
successive CPUs in a given litmus test.
There are three types of linkage:

\begin{enumerate}
\item	Write-to-read, where the next CPU reads the value that the
	previous CPU wrote.
	The LB litmus-test patterns contain only this type of relation.
	In formal memory-modeling texts, this relation is called
        ``reads-from'' and is usually abbreviated \qco{rf}.

\item	Read-to-write, where the next CPU overwrites the value that the
	previous CPU read.
	The SB litmus test contains only this type of relation.
	In formal memory-modeling texts, this relation is often called
	``from-reads'' and is sometimes abbreviated \qco{fr}.

\item	Write-to-write, where the next CPU overwrites the value written
	by the previous CPU\@.
	The Z6.0 litmus test pattern contains a write-to-write relation
	between the last access of \co{CPU1()} and the first access of
	\co{CPU2()}.
	In formal memory-modeling texts, this relation is often called
	``coherence order'' and is sometimes abbreviated \qco{co}.
	In the C++ standard, it is instead called ``modification order''
	and often abbreviated \qco{mo}.
\end{enumerate}

The strength of memory ordering required for a given litmus test to
avoid a counter-intuitive outcome depends on the types of relations
linking the memory accesses for the outcome in question:

\begin{itemize}
\item	If all links are write-to-read links, then the weakest
	possible ordering within each CPU suffices.
	For example, in the LB litmus test, a control dependency
	was enough to do the job.

\item	If all but one of the links are write-to-read links, then a
	release-acquire chain suffices.
	Both the MP and the ISA2 litmus tests illustrate this case.

\item	If more than one of the links are something other than
	write-to-read links, then a full memory barrier is required
	between each successive pair of non-write-to-read links.
	This case is illustrated by the Z6.0 litmus tests, both in the
	locking and in the release-acquire sections.
\end{itemize}

However, if you find yourself having to stretch these rules of thumb
to fit your situation, you should consider creating a litmus test and
running it on the model.
